!pip install nltk gtts moviepy Pillow requests diffusers torch transformers

import nltk
from gtts import gTTS
from moviepy.editor import ImageClip, concatenate_videoclips, AudioFileClip
from PIL import Image
import requests
from io import BytesIO
from diffusers import StableDiffusionPipeline
import torch

# Download NLTK data
nltk.download('punkt')
nltk.download('punkt_tab')

# --- Text Preprocessing ---

def preprocess_text(text):
    return nltk.sent_tokenize(text)

# --- Text-to-Speech (TTS) ---
def text_to_speech(text, output_file="output_audio.mp3"):
    tts = gTTS(text, lang='en')
    tts.save(output_file)
    return output_file
def text_to_image(text, output_file="output_image.jpg"):
    pipe = StableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16)
    pipe = pipe.to("cuda")
    image = pipe(text).images[0]
    image.save(output_file)
    return output_file

# --- Merge Images & Audio into Video ---
def create_video(image_files, audio_file, output_video="output_video.mp4"):
    clips = []
    audio_clip = AudioFileClip(audio_file)
    for img_file in image_files:
        clip = ImageClip(img_file).set_duration(3)  # 3 sec per image
        clips.append(clip)
    final_clip = concatenate_videoclips(clips).set_audio(audio_clip)
    final_clip.write_videofile(output_video, fps=24)
    return output_video

# --- Main Function ---
def text_to_video(input_text):
    sentences = preprocess_text(input_text)
    audio_file = text_to_speech(input_text)
    image_files = [text_to_image(sentence) for sentence in sentences]
    video_file = create_video(image_files, audio_file)
    print(f"âœ… Video generated: {video_file}")
text_to_video("A cat and dog fighting.")
